---
title: "Upload to S3 using AWS SDK for Java"
date: 2021-04-01T15:29:31+10:00
url: upload-to-s3-using-aws-sdk-for-java
---
![AWS S3](aws-s3.svg)

Introduction
====================

I recently had to work with S3 & AWS Java SDK for a few different file operation scenarios. I was aware of a few different approaces and learnt a few more. Surprisingly there were actually more than one right ways of using the SDK. I'll be talking about the [AWS SDK for Java 2](https://github.com/aws/aws-sdk-java-v2) using Java 11.

S3 localstack configurations
====================

![Localstack](localstack.png)

Localstack is most certainly my favourite tool for working with AWS locally. It provides a test framework for developing against AWS. Setting up Localstack for basic scenarios is easy enough using docker-compose. Stay tuned for a future post on an advanced setup post. However, on the application side we have to ensure that the S3Client is initialised correctly to work with Localstack.

> LocalstackConfiguration.java

    @Profile("!dev")
    @Configuration
    public class LocalstackConfiguration {

      @Value("${cloud.aws.region.static}")
      private String region;
      @Value("${cloud.aws.s3.endpoint:#{null}}")
      private String endpoint;
      @Value("${cloud.aws.credentials.accessKey}")
      private String accessKey;
      @Value("${cloud.aws.credentials.secretKey}")
      private String secretKey;

      @Bean
      public S3Client s3Client() {
        final AwsCredentials credentials = AwsBasicCredentials.create(accessKey, secretKey);
        AwsCredentialsProvider credentialsProvider = StaticCredentialsProvider.create(credentials);
        return S3Client.builder()
            .region(Region.of(region)).credentialsProvider(credentialsProvider)
            .serviceConfiguration(S3Configuration.builder()
                .pathStyleAccessEnabled(true).build())
            .endpointOverride(URI.create(endpoint)).build();
      }
    }

> application.yml

    cloud:
        aws:
            credentials:
            accessKey: test
            secretKey: test
            region:
            static: us-east-1
            s3:
            endpoint: http://localhost:4566

This allows us to override the endpoint the S3 client uses to now point to Localstack. accessKey & secretKey can be anything but must be set. 
The region must be consistent across all your applications but can be any valid region.

    .pathStyleAccessEnabled(true)

S3 uses a DNS style access to buckets on AWS, such that the sdk uses `bucket-name.s3.amazonaws.com/` to perform s3 operations, on localstack this would translate to `bucket-name.localstack`. However that is not a valid hostname on your local machine. So we switch to an older behaviour where the access pattern is based on paths like `s3.amazonaws.com/bucket-name` and it translates really well to `localstack/bucket-name`.

Data generation
====================

![Data Generation](data-generation.jpeg)

While most S3 uploads and download operations can be performed from and to Disk, I am going to focus on data generated by code and uploaded to S3. To achieve this data generation for testing I have used a library called EasyRandom which creates objects from a given class using random values for the fields. 

For example for the class

> SampleData.java

    @Data
    public class SampleData {
    
      private UUID uuid;
      private String string;
      private Integer integer;
      private Long long_;
      private BigDecimal bigDecimal;
      private Boolean aBoolean;
    
    }

Will generate

    {
        "uuid":"ae162e4f-e2c9-3c98-8807-78ff6dfe58e8",
        "string":"GQpz",
        "integer":-83529235,
        "long_":2137344153351374497,
        "bigDecimal":0.11586566171997125795911642853752709925174713134765625,
        "aboolean":true
    }

by simply calling the method

    new EasyRandom().nextObject(SampleData.class)

Simple S3 upload
====================

The simplest way to upload to s3 is to use the putObject method

    public Long simpleFileUpload(String key, Integer count) throws JsonProcessingException {
      Stream<SampleData> sampleDataStream = generator.objects(SampleData.class, count);
      String sampleDataJson = objectMapper.writeValueAsString(sampleDataStream.collect(
          Collectors.toList()));
      RequestBody requestBody = RequestBody
          .fromBytes(sampleDataJson.getBytes());
  
      s3Client.putObject(PutObjectRequest.builder()
          .key(key)
          .bucket(bucketName)
          .build(), requestBody);
      HeadObjectResponse headObjectResponse = s3Client.headObject(HeadObjectRequest.builder()
          .bucket(bucketName)
          .key(key)
          .build());
      return headObjectResponse.contentLength();
    }

This method creates N objects and then serialises them into a json array. This final output is then sent to S3 putObject. We then query the size of the uploaded object and return it. While this is a simple technique the downsides are quite obvious. We are limited by the size of available heap size. We are also very likely to hit OOMs for large files.


S3 upload with an intermediary file
====================

Since the SDK supports uploading from a file, we can always use an intermediary file between the data generation and file upload. However this only solves the memory problem to some extent. The total time increases because we are now additionally writing to a file and then reading from the file before uploading to s3. However this could be a quick solution if you have access to enough disk space. This however is not ideal when running on lambda's or Kubernetes or if you want to optimise for time.
With this change I was able to upload larger files to Localstack, however localstack seems to have a bug or limitation of around 320MB per single upload request. With AWS I was able to upload XXX MB file in XX Minutes XX Seconds. 
The reason we see this improvement is only because we have now separated the steps where there are a lot of objects in memory and when we load the entire contents of the file in memory. Giving the JVM some time to garbage collect. However this comes at the cost of Slower over all performance. 
There are only a few minor tweaks we need to do to make the simple Upload use an intermediary file. Assuming the data is written to disk with the same filename.

    public Long simpleFileUploadWithFile(String key) throws IOException {
      PutObjectResponse putObjectResponse = s3Client.putObject(PutObjectRequest.builder()
          .key(key)
          .bucket(bucketName)
          .build(), Paths.get(key));
  
      HeadObjectResponse headObjectResponse = s3Client.headObject(HeadObjectRequest.builder()
          .bucket(bucketName)
          .key(key)
          .build());
      return headObjectResponse.contentLength();
  
    }

By writing each line into the file we never use more memory than there is available.

    JsonFactory jfactory = new JsonFactory();

    JsonGenerator jGenerator = jfactory
        .createGenerator(Paths.get("json.json").toFile(), JsonEncoding.UTF8);
    jGenerator.setCodec(objectMapper);

    jGenerator.writeStartArray();
    sampleDataStream.forEach(pojo -> {
      try {
        jGenerator.writeObject(pojo);
      } catch (IOException e) {
        e.printStackTrace();
      }
    });

    jGenerator.writeEndArray();

While this ensures that we will not encounter an OOM, it introduces unnecessary steps of file writes and reads.  

Conclusion
====================



Next Steps
====================

To efficiently upload files larger than 100mb we then try to use the Multi part upload feature provided in the SDK in the next Part. 

Have you used S3 or any alternatives? Please share in the comments about your experience.